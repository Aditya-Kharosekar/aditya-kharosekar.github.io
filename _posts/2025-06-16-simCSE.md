---
layout: post
title: Contrastive learning to fine-tune embeddings
subtitle: SimCSE - Simple Contrastive Learning of Sentence Embeddings
tags: [paper reading, embeddings, unsupervised]
---

Arxiv link: [SimCSE - Simple Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2104.08821)

They used contrastive learning to greatly improve pre-trained embeddings as measure by performance on semantic textual similarity tasks.

I wanted to read this paper because I've seen it cited a few times in other embeddings papers, and I wanted to double-down on upping my embeddings knowledge because I would like that to be a USP for me.

## Outline and major points

**Contrastive learning**

* The objective of embeddings is to have semantically similar content close to each other in the embeddings space, and have semantically dis-similar content far apart. Contrastive learning is a way of doing this. It’s quite common in computer vision apparently.
* It feeds the model positive examples (sentences that are semantically related and should be close together in the embedding space) and negative examples (sentences that are not semantically related and should be far apart in the embeddings space). The idea is that this trains the model on what is similar and equally importantly, what is not similar.
* This approach needs a way of creating positive examples. Previous approaches have been: randomly delete a word from the original sentence, create pairs from adjacent sentences.

**Unsupervised SimCSE**

* They create positive examples by passing the same sentence through two random dropout masks. Dropout acts as a form of data augmentation.
* They use a cross-entropy training objective. Which means that the embeddings for x1 and x1’ (x1 passed through a random dropout mask) are incentivized to be close to each other.
* They use the other sentences in the batch as negative examples

**Supervised SimCSE**

* They created positive examples from multiple datasets which have sentence-pair examples. From the datasets they ended up picking, they created positive examples by taking each “premise” and its “entailment” sentence, and negative examples by taking each “premise” and its “contradiction” sentence.

**Performance**
They performed experiments on some Semantic Textual Similarity (STS) tasks, i.e. they took their fine-tuned embeddings that used SimCSE vs other approaches, and they got better numbers. I wasnt super interested in this because I was more interested in the approach.

**Other cool concepts I learned from this paper:**

* Two ways to calculate the quality of embeddings produced by a method are:
    * Alignment: The average distance between the embeddings of positive pairs. This should be low because we want similar texts to be close by.
    * Uniformity: how evenly distributed all embeddings are if calculate distances between random texts. We want this to be evenly distributed across the space.
* Anisotropy: the fact that embeddings are not evenly distributed in the vector space, but actually are concentrated in a narrow cone. This can reduce their effectiveness in distinguishing between texts because everything is grouped close by.


## What I would like to read up on next
* I need to recap cross-entropy loss
* I want to read one of the STS task papers to have a clearer idea of what the tasks look like
* It would be nice to read an applied paper on how embeddings have been used in industry